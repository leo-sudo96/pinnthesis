{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "95fe04c0-8d64-4c65-aca6-c18604e7173b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from scipy.integrate import odeint\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.integrate import odeint\n",
    "from torch import nn, autograd\n",
    "import random\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import tqdm\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4ced1cd4-61aa-42fc-acc8-e50bdd492f17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Define the device globally\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)\n",
    "current_directory = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "19f97bce-1731-4698-bffb-5481e09ddff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_result(x, y_tensor_squeezed, x_data, y_data, yh, xp=None, physics_params=None, model=None):\n",
    "    \"\"\"\n",
    "    Plots training data, ground truth, model predictions, and optionally physics predictions.\n",
    "\n",
    "    Parameters:\n",
    "    - x: The full range of x values for ground truth data.\n",
    "    - y_tensor_squeezed: Ground truth data corresponding to x.\n",
    "    - x_data: X values of the training data.\n",
    "    - y_data: Y values of the training data (observed outputs).\n",
    "    - yh: Model predictions corresponding to x_data.\n",
    "    - xp: Optional, additional x points for physics-based predictions.\n",
    "    - physics_params: Optional, physics parameters for model if physics predictions are desired.\n",
    "    - model: The trained model, required if physics_params and xp are provided.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    # Plot training data\n",
    "    plt.scatter(x_data.detach().numpy(), y_data.detach().numpy(), color=\"tab:orange\", label=\"Training data\")\n",
    "    \n",
    "    # Plot ground truth data\n",
    "    plt.plot(x.detach().numpy(), y_tensor_squeezed.detach().numpy(), 'r-', label='Ground Truth Data')\n",
    "    \n",
    "    # Plot model predictions\n",
    "    plt.plot(x_data.detach().numpy(), yh.detach().numpy(), 'b--', label='Model Predictions')\n",
    "    \n",
    "    # Optionally, plot physics-based predictions\n",
    "    if xp is not None and physics_params is not None and model is not None:\n",
    "        # Assuming model can directly use xp and physics_params for prediction\n",
    "        # You might need to adjust this call based on how your model uses physics_params\n",
    "        yhp = model(torch.cat((xp, torch.tensor(physics_params).repeat(len(xp), 1)), dim=1)).detach()\n",
    "        plt.plot(xp.detach().numpy(), yhp.numpy(), 'g:', label='Physics Predictions')\n",
    "    \n",
    "    plt.xlabel('X Value')\n",
    "    plt.ylabel('Y Value')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "23d98d9a-9f3e-4b9b-9499-ccfc40abe823",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OscillationDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data, targets):#, boundary_conditions\n",
    "        self.data = data\n",
    "        self.targets = targets\n",
    "        #self.boundary_conditions = boundary_conditions\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.data[idx]\n",
    "        target = self.targets[idx]\n",
    "        #boundary_condition = self.boundary_conditions[idx]\n",
    "        return sample, target#, boundary_condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "38a2a7a2-ae3c-4324-97ac-f3067e4b2ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data_with_boundaries(num_samples, x):\n",
    "    data = []\n",
    "    #boundary_values = []\n",
    "    f_boundary_values = []  # This might be used differently, depending on what exactly you need\n",
    "    \n",
    "    # Ensure x is a numpy array for compatibility with odeint\n",
    "    x_np = x.numpy() if isinstance(x, torch.Tensor) else x\n",
    "\n",
    "    for _ in range(num_samples):\n",
    "        # Randomly choose parameters for each sample\n",
    "        a = random.uniform(-2, 2)\n",
    "        b = random.uniform(0, 3)\n",
    "        d = random.uniform(0, 0.5)\n",
    "        gamma = random.uniform(0, 1.5)\n",
    "        omega = random.uniform(0, 2.5)\n",
    "\n",
    "        # Define initial conditions\n",
    "        #y0 = [random.uniform(-1, 1), random.uniform(-1, 1)]\n",
    "        y0 = [0, 0]\n",
    "        # Solve the Duffing oscillator equation\n",
    "        # Local definition of the differential equation with captured parameters\n",
    "        \n",
    "        def duffing(y, t, a=a, b=b, d=d, gamma=gamma, omega=omega):\n",
    "            x, x_dot = y\n",
    "            d2x_dt2 = -d * x_dot - a * x - b * x**3 + gamma * np.cos(omega * t)\n",
    "            return [x_dot, d2x_dt2]\n",
    "\n",
    "        sol = odeint(duffing, y0, x_np, args=(a, b, d, gamma, omega))\n",
    "        x_t = sol[:, 0]  # Solution x(t)\n",
    "       \n",
    "        # Compile the parameters and outputs to form the dataset\n",
    "        for xi, xti in zip(x_np, x_t):\n",
    "            data.append([xi, a, b, d, gamma, omega, xti])\n",
    "            \n",
    "        \n",
    "    return np.array(data)#, np.array(boundary_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "05b238a5-39ff-45e7-82ae-8b5a147eb2fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data, boundary_values = generate_data_with_boundaries(num_samples=10000, x=np.linspace(0, 10, 500))\n",
    "data= generate_data_with_boundaries(num_samples=1000000, x=np.linspace(0, 10, 5000))\n",
    "# Convert the generated data to PyTorch tensors\n",
    "X = torch.tensor(data[:, :-1], dtype=torch.float32)  # Input features: [x_i, a, b, d, gamma, omega]\n",
    "Y = torch.tensor(data[:, -1], dtype=torch.float32).view(-1, 1)  # Targets: x(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7b77eb9f-973c-4331-a906-649fef154fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming data generation and conversion to tensors has been done as previously described\n",
    "X_tensor = torch.tensor(data[:, :-1], dtype=torch.float32)#.requires_grad_(True)  # Features: [x_i, a, b, d, gamma, omega]\n",
    "Y_tensor = torch.tensor(data[:, -1], dtype=torch.float32).view(-1, 1)  # Targets: x(t_i)\n",
    "# Set up the boundary conditions\n",
    "X_BOUNDARY = [0.0]  # boundary condition coordinate\n",
    "F_BOUNDARY = [0.0]  # boundary condition value\n",
    "\n",
    "x_boundary = torch.tensor([X_BOUNDARY]).requires_grad_(True)\n",
    "f_boundary = torch.tensor([F_BOUNDARY]).requires_grad_(True)\n",
    "\n",
    "# Assuming boundary_conditions is prepared alongside data and targets\n",
    "oscillation_dataset = OscillationDataset(X_tensor, Y_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dda4e872-105f-4fe4-9ff2-adf710589df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming boundary_conditions is prepared alongside data and targets\n",
    "oscillation_dataset = OscillationDataset(X_tensor, Y_tensor)#, boundary_conditions_tensor\n",
    "\n",
    "\n",
    "oscillation_dataloader = DataLoader(oscillation_dataset, batch_size=2048, shuffle=True, num_workers=4)#, collate_fn=custom_collate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "863cc0ae-c678-4398-9a5d-8eb154c2541b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define a class to create a fully connected neural network\n",
    "class FCN(nn.Module):\n",
    "    def __init__(self, N_INPUT, N_OUTPUT, N_HIDDEN, N_LAYERS):\n",
    "        super().__init__()\n",
    "        activation = nn.Tanh\n",
    "        self.fcs = nn.Sequential(\n",
    "            nn.Linear(N_INPUT, N_HIDDEN),\n",
    "            activation()\n",
    "        )\n",
    "        self.fch = nn.Sequential(\n",
    "            *[nn.Sequential(\n",
    "                nn.Linear(N_HIDDEN, N_HIDDEN),\n",
    "                activation()\n",
    "            ) for _ in range(N_LAYERS-1)]\n",
    "        )\n",
    "        self.fce = nn.Linear(N_HIDDEN, N_OUTPUT)\n",
    "        # Assuming your linear layer is named `layer`\n",
    "        input_size = self.fce.in_features\n",
    "        # print(\"Input size of the linear layer: \", input_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fcs(x)\n",
    "        x = self.fch(x)\n",
    "        x = self.fce(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2713eed9-6330-4d08-9bac-d70fce5f17f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_INPUT = 6 # [x_i, a, b, d, gamma, omega]\n",
    "N_HIDDEN = 128\n",
    "N_OUTPUT = 1  # x(t)\n",
    "N_LAYERS = 4\n",
    "epochs = 100000\n",
    "model = FCN(N_INPUT, N_OUTPUT, N_HIDDEN, N_LAYERS)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0003)\n",
    "model = model.to(device)\n",
    "tensor_list = [X_tensor, Y_tensor, x_boundary, f_boundary]  # Your tensors\n",
    "tensor_list = [t.to(device) for t in tensor_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a2a364b9-edfd-4b05-965a-42d530ba789e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#initial_lr = 0.001\n",
    "#num_warmup_steps = 1000\n",
    "#num_total_steps = 6000\n",
    "#decay_rate = 0.1\n",
    "#decay_steps = 100\n",
    "\n",
    "#def lr_lambda(current_step: int):\n",
    "#    if current_step < num_warmup_steps:\n",
    "        # Linear warmup\n",
    "#        return float(current_step) / float(max(1, num_warmup_steps))\n",
    "#    else:\n",
    "        # Exponential decay\n",
    "#        return decay_rate ** ((current_step - num_warmup_steps) // decay_steps)\n",
    "\n",
    "# Define the scheduler\n",
    "#scheduler = LambdaLR(optimizer, lr_lambda)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ded00e1b-1f73-4eb8-ae11-e84ef6d39573",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PhysicsInformedLoss(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, yhp, data, d, a, b, gamma, omega):\n",
    "        # Forward pass computations\n",
    "        ctx.save_for_backward(data, d, a, b, gamma, omega, yhp)\n",
    "        # Loss computation\n",
    "        return loss_physics\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        # Retrieve saved tensors\n",
    "        data, d, a, b, gamma, omega, yhp = ctx.saved_tensors\n",
    "        \n",
    "        # Example gradient computations (conceptual and simplified)\n",
    "        # These would need to be replaced with the actual gradients based on your loss function\n",
    "        grad_yhp = torch.autograd.grad(physics_loss, yhp, grad_outputs=grad_output, create_graph=True)[0]\n",
    "        grad_data = torch.autograd.grad(physics_loss, data, grad_outputs=grad_output, create_graph=True)[0]\n",
    "\n",
    "        # Assume gradients w.r.t. other parameters are not required\n",
    "        return grad_yhp, grad_data, None, None, None, None, None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bc372d0e-57a4-4793-9358-0f30fb98976b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Total Loss: 0.10001169145107269\n",
      "Model saved to /home/leo/devel/thesis/rnn_pinnthesis/fcn_pinn_1sec_higher_varparam_model.pth\n",
      "Epoch 1, Total Loss: 0.017177987843751907\n",
      "Epoch 2, Total Loss: 0.003629024140536785\n",
      "Epoch 3, Total Loss: 0.0031155566684901714\n",
      "Epoch 4, Total Loss: 0.002432542620226741\n",
      "Epoch 5, Total Loss: 0.0020698769949376583\n",
      "Epoch 6, Total Loss: 0.0016371742822229862\n",
      "Epoch 7, Total Loss: 0.0012841284042224288\n",
      "Epoch 8, Total Loss: 0.0010492915753275156\n",
      "Epoch 9, Total Loss: 0.0007726329495199025\n",
      "Epoch 10, Total Loss: 0.0006008592317812145\n",
      "Model saved to /home/leo/devel/thesis/rnn_pinnthesis/fcn_pinn_1sec_higher_varparam_model.pth\n",
      "Epoch 11, Total Loss: 0.0005366507102735341\n",
      "Epoch 12, Total Loss: 0.0004788600781466812\n",
      "Epoch 13, Total Loss: 0.000487249722937122\n",
      "Epoch 14, Total Loss: 0.0004321751475799829\n",
      "Epoch 15, Total Loss: 0.0004472017171792686\n",
      "Epoch 16, Total Loss: 0.0004494248714763671\n",
      "Epoch 17, Total Loss: 0.00041781843174248934\n",
      "Epoch 18, Total Loss: 0.00042144584585912526\n",
      "Epoch 19, Total Loss: 0.00042112189112231135\n",
      "Epoch 20, Total Loss: 0.0004039648047182709\n",
      "Model saved to /home/leo/devel/thesis/rnn_pinnthesis/fcn_pinn_1sec_higher_varparam_model.pth\n",
      "Epoch 21, Total Loss: 0.0003998404718004167\n",
      "Epoch 22, Total Loss: 0.0004053123411722481\n",
      "Epoch 23, Total Loss: 0.00040306898881681263\n",
      "Epoch 24, Total Loss: 0.0004033876466564834\n",
      "Epoch 25, Total Loss: 0.00039532064693048596\n",
      "Epoch 26, Total Loss: 0.00040328127215616405\n",
      "Epoch 27, Total Loss: 0.0004052560543641448\n",
      "Epoch 28, Total Loss: 0.00040748456376604736\n",
      "Epoch 29, Total Loss: 0.00040754873771220446\n",
      "Epoch 30, Total Loss: 0.00039343873504549265\n",
      "Model saved to /home/leo/devel/thesis/rnn_pinnthesis/fcn_pinn_1sec_higher_varparam_model.pth\n",
      "Epoch 31, Total Loss: 0.00044042940135113895\n",
      "Epoch 32, Total Loss: 0.0003923046460840851\n",
      "Epoch 33, Total Loss: 0.00039407360600307584\n",
      "Epoch 34, Total Loss: 0.0004056836769450456\n",
      "Epoch 35, Total Loss: 0.0003995579027105123\n",
      "Epoch 36, Total Loss: 0.0003899582370650023\n",
      "Epoch 37, Total Loss: 0.00040683025144971907\n",
      "Epoch 38, Total Loss: 0.0004028378752991557\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 42\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[38;5;66;03m# Combined loss\u001b[39;00m\n\u001b[1;32m     40\u001b[0m     total_loss \u001b[38;5;241m=\u001b[39m loss_physics \u001b[38;5;241m+\u001b[39m loss_data \u001b[38;5;241m+\u001b[39m loss_boundary \n\u001b[0;32m---> 42\u001b[0m     \u001b[43mtotal_loss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     43\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     44\u001b[0m scheduler\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/anaconda3/envs/thesis/lib/python3.11/site-packages/torch/_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    521\u001b[0m     )\n\u001b[0;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/thesis/lib/python3.11/site-packages/torch/autograd/__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 266\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in tqdm.tqdm(range(epochs)):\n",
    "    for batch_idx, (sample, targets) in enumerate(oscillation_dataloader):#, batch_boundary_conditions\n",
    "        optimizer.zero_grad()\n",
    "         # Set requires_grad to True for data\n",
    "        sample.requires_grad_(True)\n",
    "        data = sample.to(device)\n",
    "        \n",
    "        targets = targets.to(device)\n",
    "        # Model prediction for the full dataset\n",
    "        yh = model(data)\n",
    "            # Data loss (comparing model output with true data)\n",
    "        y_data = Y_tensor  # True output values from your dataset\n",
    "        loss_data = torch.mean((yh - targets)**2)\n",
    "        \n",
    "        # Extract domain (time) and parameters from X_tensor\n",
    "        x_domain = data[:, 0].view(-1, 1).requires_grad_(True)\n",
    "        params = data[:, 1:]  # Parameters: a, b, d, gamma, omega\n",
    "        params = params.to(device)\n",
    "        a, b, d, gamma, omega = params.t()  # Transpose for convenience in calculations\n",
    "    \n",
    "        yhp = model(data)\n",
    "        dy_pred = torch.autograd.grad(yhp, data, torch.ones_like(yhp), create_graph=True)[0]\n",
    "        d2y_pred = torch.autograd.grad(dy_pred, data, torch.ones_like(dy_pred), create_graph=True)[0]\n",
    "\n",
    "        physics_loss = d2y_pred + d.unsqueeze(1) * dy_pred + a.unsqueeze(1) * yhp + b.unsqueeze(1) * torch.pow(yhp, 3) - gamma.unsqueeze(1) * torch.cos(omega.unsqueeze(1) * data)\n",
    "        loss_physics = (1e-4) * torch.mean(torch.square(physics_loss))\n",
    "       \n",
    "        x_boundary = torch.tensor([X_BOUNDARY]).requires_grad_(True)\n",
    "        x_boundary =  x_boundary.repeat(params.size(0),1)\n",
    "        x_boundary = x_boundary.to(device)\n",
    "        x_boundary = torch.cat([x_boundary,params],dim=1)\n",
    "        f_boundary = torch.tensor([F_BOUNDARY]).requires_grad_(True)\n",
    "        f_boundary = f_boundary.to(device)\n",
    "        yh_boundary = model(x_boundary)\n",
    "        boundary = yh_boundary - f_boundary\n",
    "        loss_boundary = (1e-6) * torch.mean(boundary**2)\n",
    "    \n",
    "        \n",
    "        # Combined loss\n",
    "        total_loss = loss_physics + loss_data + loss_boundary \n",
    "    \n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "    #scheduler.step()\n",
    "    if epoch % 1 == 0:\n",
    "        print(f'Epoch {epoch}, Total Loss: {total_loss.item()}')\n",
    "\n",
    "    if  epoch % 10 == 0:\n",
    "        model_save_path = os.path.join(current_directory, \"/models/fcn_pinn_1sec_higher_varparam_model.pth\")\n",
    "        torch.save(model.state_dict(), model_save_path)\n",
    "        print(f\"Model saved to {model_save_path}\")\n",
    "            \n",
    "    if total_loss < 1e-7:\n",
    "        model_save_path = os.path.join(current_directory, \"/models/fcn_pinn_1sec_higher_varparam_model.pth\")\n",
    "        torch.save(model.state_dict(), model_save_path)\n",
    "        print(f\"Model saved to {model_save_path}\")\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c0971cb-6fa1-4087-9f1a-bd666d522c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting results\n",
    "        plot_result(x=X_tensor[:, 0],  # Domain (time points)\n",
    "                    y_tensor_squeezed=Y_tensor,  # Ground truth data\n",
    "                    x_data=X_tensor[:, 0],  # Same as domain for plotting\n",
    "                    y_data=Y_tensor,  # Ground truth data for scatter plot\n",
    "                    yh= yh,  # Model predictions\n",
    "                    xp=None,  # Additional physics-based prediction points, if applicable\n",
    "                    physics_params=None,  # Physics parameters, if needed for additional predictions\n",
    "                    model= yhp)  # Model, if additional physics-based predictions are to be plotted    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "319b307d-e97d-4991-96f7-d63db0aec604",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
