{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e79351d3-c323-4d4c-a41f-9d7ebd8c317e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6d5d995a-14bd-486b-9b92-28aaedf4be01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The jupyter_ai extension is already loaded. To reload it, use:\n",
      "  %reload_ext jupyter_ai\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from scipy.integrate import odeint\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.integrate import odeint\n",
    "from torch import nn, autograd\n",
    "from src.transformer_nn import TransformerModel\n",
    "#from src.transformer_nn import SimpleLinearModel\n",
    "from src.duffing_generator import DuffingGeneratorClass\n",
    "from src.physics_loss import physics_loss_class\n",
    "import torch.optim as op \n",
    "%load_ext jupyter_ai\n",
    "import torch.optim as optim\n",
    "#from torch.utils.data import Data\n",
    "import transformers #import Transformer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4b02c230-6034-4bd0-b397-8e369c72ff2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.linspace(0, 1, 1000).view(-1, 1)\n",
    "dg = DuffingGeneratorClass()  # Instantiate your class\n",
    "\n",
    "device = 'cpu'\n",
    "torch.manual_seed(123)\n",
    "physics_loss_instance = physics_loss_class()\n",
    "# Set up the physics loss training locations\n",
    "x_physics = torch.linspace(0, 1, 15).view(-1, 1).requires_grad_(True)\n",
    "X_BOUNDARY = 0.0  # Example value, adjust as needed\n",
    "F_BOUNDARY = 0.0  # Example value, adjust as needed\n",
    "n_time_features = 1  # Assuming the time component is a single feature\n",
    "n_param_features = 5  # Assuming there are 5 other parameter features\n",
    "n_hidden = 128\n",
    "n_layers = 8\n",
    "n_heads = 4\n",
    "# Initialize model, optimizer, and loss function\n",
    "model = TransformerModel(n_time_features, n_param_features, n_hidden, n_layers, n_heads).to(device)\n",
    "# Example: Learning rate scheduler\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "loss_instance = physics_loss_class()\n",
    "#scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.5)\n",
    "num_epochs = 100  # Adjust as needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0d9d8885-d478-424a-bff0-389165c23c32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# Create a small batch of test data with varying time\\ntest_time = torch.linspace(0, 1, 10).view(-1, 1)  # 10 time steps\\ntest_params = torch.zeros(10, 5)  # Keep other parameters constant\\ntest_x_combined = torch.cat((test_time, test_params), dim=1).to(device)\\n\\n# Get model\\'s predictions for this test data\\nmodel.eval()  # Set the model to evaluation mode\\nwith torch.no_grad():\\n    test_output = model(test_x_combined)\\n    print(\"Model output for test data:\", test_output)'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''# Create a small batch of test data with varying time\n",
    "test_time = torch.linspace(0, 1, 10).view(-1, 1)  # 10 time steps\n",
    "test_params = torch.zeros(10, 5)  # Keep other parameters constant\n",
    "test_x_combined = torch.cat((test_time, test_params), dim=1).to(device)\n",
    "\n",
    "# Get model's predictions for this test data\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "with torch.no_grad():\n",
    "    test_output = model(test_x_combined)\n",
    "    print(\"Model output for test data:\", test_output)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60d4e330-214b-4833-9af9-ae0703c57d20",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0, Total Loss: 0.007273682858794928\n",
      "Batch 1, Total Loss: 0.027515480294823647\n",
      "Batch 2, Total Loss: 0.0073003401048481464\n",
      "Batch 3, Total Loss: 0.01609214022755623\n",
      "Batch 4, Total Loss: 0.0074824439361691475\n",
      "Batch 5, Total Loss: 0.02104845456779003\n",
      "Batch 6, Total Loss: 0.013806751929223537\n",
      "Batch 7, Total Loss: 0.020815791562199593\n",
      "Batch 8, Total Loss: 0.016188398003578186\n",
      "Batch 9, Total Loss: 0.011843899264931679\n",
      "Batch 10, Total Loss: 7.949159044073895e-05\n",
      "Batch 11, Total Loss: 0.003177050966769457\n",
      "Batch 12, Total Loss: 0.011062638834118843\n",
      "Batch 13, Total Loss: 0.00020531962218228728\n",
      "Batch 14, Total Loss: 0.00016681004490237683\n",
      "Batch 15, Total Loss: 0.002340924460440874\n",
      "Batch 16, Total Loss: 0.0016657405067235231\n",
      "Batch 17, Total Loss: 0.0002608431095723063\n",
      "Batch 18, Total Loss: 0.019579026848077774\n",
      "Batch 19, Total Loss: 0.0008842418901622295\n",
      "Batch 20, Total Loss: 0.0001360338064841926\n",
      "Batch 21, Total Loss: 6.24820459051989e-05\n",
      "Batch 22, Total Loss: 0.015589752234518528\n",
      "Batch 23, Total Loss: 0.00948240328580141\n",
      "Batch 24, Total Loss: 0.016772886738181114\n",
      "Batch 25, Total Loss: 0.00027541667805053294\n",
      "Batch 26, Total Loss: 0.0016916913446038961\n",
      "Batch 27, Total Loss: 0.010437482967972755\n",
      "Batch 28, Total Loss: 0.0001660423877183348\n",
      "Batch 29, Total Loss: 0.0007064543315209448\n",
      "Batch 30, Total Loss: 0.013055825605988503\n",
      "Batch 31, Total Loss: 0.002952439244836569\n",
      "Batch 32, Total Loss: 0.002560063498094678\n",
      "Batch 33, Total Loss: 0.0014829576248303056\n",
      "Batch 34, Total Loss: 0.011028078384697437\n",
      "Batch 35, Total Loss: 0.0051847193390131\n",
      "Batch 36, Total Loss: 0.000823704875074327\n",
      "Batch 37, Total Loss: 0.011146302334964275\n",
      "Batch 38, Total Loss: 0.013215158134698868\n",
      "Batch 39, Total Loss: 2.1614873730868567e-06\n",
      "Batch 40, Total Loss: 0.0015991737600415945\n",
      "Batch 41, Total Loss: 0.012881539762020111\n",
      "Batch 42, Total Loss: 0.0012047170894220471\n",
      "Batch 43, Total Loss: 0.004258385393768549\n",
      "Batch 44, Total Loss: 0.0007636899827048182\n",
      "Batch 45, Total Loss: 0.005222270265221596\n",
      "Batch 46, Total Loss: 0.00015759233792778105\n",
      "Batch 47, Total Loss: 0.0008782210061326623\n",
      "Batch 48, Total Loss: 0.0037740138359367847\n",
      "Batch 49, Total Loss: 0.012419485487043858\n",
      "Batch 50, Total Loss: 0.01427241787314415\n",
      "Batch 51, Total Loss: 0.0005727295065298676\n",
      "Batch 52, Total Loss: 0.0017606883775442839\n",
      "Batch 53, Total Loss: 0.0017030062153935432\n",
      "Batch 54, Total Loss: 0.008481421507894993\n",
      "Batch 55, Total Loss: 0.009958578273653984\n",
      "Batch 56, Total Loss: 0.0020637568086385727\n",
      "Batch 57, Total Loss: 0.004120024852454662\n",
      "Batch 58, Total Loss: 0.0009456976549699903\n",
      "Batch 59, Total Loss: 0.013082091696560383\n",
      "Batch 60, Total Loss: 0.00703087542206049\n",
      "Batch 61, Total Loss: 0.00471878657117486\n",
      "Batch 62, Total Loss: 0.005616801790893078\n",
      "Batch 63, Total Loss: 0.012190318666398525\n",
      "Batch 64, Total Loss: 0.0002947600733023137\n",
      "Batch 65, Total Loss: 0.003907615784555674\n",
      "Batch 66, Total Loss: 0.001611130777746439\n",
      "Batch 67, Total Loss: 6.176417082315311e-05\n",
      "Batch 68, Total Loss: 0.01659553125500679\n",
      "Batch 69, Total Loss: 0.007529544178396463\n",
      "Batch 70, Total Loss: 0.0002715833834372461\n",
      "Batch 71, Total Loss: 0.0034124392550438643\n",
      "Batch 72, Total Loss: 0.007442736532539129\n",
      "Batch 73, Total Loss: 0.008058983832597733\n",
      "Batch 74, Total Loss: 0.00914717372506857\n",
      "Batch 75, Total Loss: 0.00028108031256124377\n",
      "Batch 76, Total Loss: 0.0029434924945235252\n",
      "Batch 77, Total Loss: 0.0022917387541383505\n",
      "Batch 78, Total Loss: 0.0010746419429779053\n",
      "Batch 79, Total Loss: 0.000757553381845355\n",
      "Batch 80, Total Loss: 0.012014038860797882\n",
      "Batch 81, Total Loss: 0.0024630550760775805\n",
      "Batch 82, Total Loss: 0.001021197298541665\n",
      "Batch 83, Total Loss: 0.000595029559917748\n",
      "Batch 84, Total Loss: 0.0020489287562668324\n",
      "Batch 85, Total Loss: 0.008294140920042992\n",
      "Batch 86, Total Loss: 0.002230386482551694\n",
      "Batch 87, Total Loss: 0.0035061067901551723\n",
      "Batch 88, Total Loss: 0.002509218640625477\n",
      "Batch 89, Total Loss: 0.0005597989074885845\n",
      "Batch 90, Total Loss: 0.009336577728390694\n",
      "Batch 91, Total Loss: 0.0020617954432964325\n",
      "Batch 92, Total Loss: 0.008210452273488045\n",
      "Batch 93, Total Loss: 0.011996787041425705\n",
      "Batch 94, Total Loss: 0.008099458180367947\n",
      "Batch 95, Total Loss: 0.0015524487243965268\n",
      "Batch 96, Total Loss: 0.0063151041977107525\n",
      "Batch 97, Total Loss: 0.0027142702601850033\n",
      "Batch 98, Total Loss: 0.0014163852902129292\n",
      "Batch 99, Total Loss: 0.000659165030810982\n",
      "Batch 0, Total Loss: 0.003064574906602502\n",
      "Batch 1, Total Loss: 0.0007995979394763708\n",
      "Batch 2, Total Loss: 0.0006185052916407585\n",
      "Batch 3, Total Loss: 0.0017102895071730018\n",
      "Batch 4, Total Loss: 0.003447821596637368\n",
      "Batch 5, Total Loss: 0.0002848475123755634\n",
      "Batch 6, Total Loss: 0.003543857019394636\n",
      "Batch 7, Total Loss: 0.00036658867611549795\n",
      "Batch 8, Total Loss: 0.000478228903375566\n",
      "Batch 9, Total Loss: 0.008013200014829636\n",
      "Batch 10, Total Loss: 0.015234477818012238\n",
      "Batch 11, Total Loss: 0.0002837509091477841\n",
      "Batch 12, Total Loss: 0.0008640114683657885\n",
      "Batch 13, Total Loss: 0.009682641364634037\n",
      "Batch 14, Total Loss: 0.007585944142192602\n",
      "Batch 15, Total Loss: 0.0001295153342653066\n",
      "Batch 16, Total Loss: 0.003272024681791663\n",
      "Batch 17, Total Loss: 0.00452097924426198\n",
      "Batch 18, Total Loss: 0.0008039569365791976\n",
      "Batch 19, Total Loss: 0.006976699456572533\n",
      "Batch 20, Total Loss: 0.008715242147445679\n",
      "Batch 21, Total Loss: 4.922728840028867e-05\n",
      "Batch 22, Total Loss: 0.0009125587530434132\n",
      "Batch 23, Total Loss: 0.0031945924274623394\n",
      "Batch 24, Total Loss: 0.0003953399136662483\n",
      "Batch 25, Total Loss: 0.0026922577526420355\n",
      "Batch 26, Total Loss: 0.000564325659070164\n",
      "Batch 27, Total Loss: 0.0047159804962575436\n",
      "Batch 28, Total Loss: 0.006603621877729893\n",
      "Batch 29, Total Loss: 0.004276410210877657\n",
      "Batch 30, Total Loss: 0.00020510544709395617\n",
      "Batch 31, Total Loss: 0.0015857568942010403\n",
      "Batch 32, Total Loss: 0.003982205875217915\n",
      "Batch 33, Total Loss: 0.0016018834430724382\n",
      "Batch 34, Total Loss: 0.01277446560561657\n",
      "Batch 35, Total Loss: 0.000223393872147426\n",
      "Batch 36, Total Loss: 0.0017071771435439587\n",
      "Batch 37, Total Loss: 0.0012656885664910078\n",
      "Batch 38, Total Loss: 0.0013474272564053535\n",
      "Batch 39, Total Loss: 0.00036839774111285806\n",
      "Batch 40, Total Loss: 0.003598202019929886\n",
      "Batch 41, Total Loss: 0.002403401071205735\n",
      "Batch 42, Total Loss: 0.0034182576928287745\n",
      "Batch 43, Total Loss: 0.0018243714002892375\n",
      "Batch 44, Total Loss: 0.010511399246752262\n",
      "Batch 45, Total Loss: 0.0003923991753254086\n",
      "Batch 46, Total Loss: 0.009686226956546307\n",
      "Batch 47, Total Loss: 0.00635931920260191\n",
      "Batch 48, Total Loss: 0.00030224205693230033\n",
      "Batch 49, Total Loss: 0.004991671070456505\n",
      "Batch 50, Total Loss: 0.00019445603538770229\n",
      "Batch 51, Total Loss: 0.0017800380010157824\n",
      "Batch 52, Total Loss: 0.0003302774566691369\n",
      "Batch 53, Total Loss: 0.0018448616610839963\n",
      "Batch 54, Total Loss: 0.0007767159258946776\n",
      "Batch 55, Total Loss: 0.0002683488419279456\n",
      "Batch 56, Total Loss: 0.007951855659484863\n",
      "Batch 57, Total Loss: 0.0005846807616762817\n",
      "Batch 58, Total Loss: 0.00012084696209058166\n",
      "Batch 59, Total Loss: 0.012182827107608318\n",
      "Batch 60, Total Loss: 0.004491677973419428\n",
      "Batch 61, Total Loss: 0.00024950833176262677\n",
      "Batch 62, Total Loss: 0.0008162670419551432\n",
      "Batch 63, Total Loss: 0.0005152393714524806\n",
      "Batch 64, Total Loss: 8.48401541588828e-05\n",
      "Batch 65, Total Loss: 0.0006311343749985099\n",
      "Batch 66, Total Loss: 0.0023625395260751247\n",
      "Batch 67, Total Loss: 0.0003774942015297711\n",
      "Batch 68, Total Loss: 0.0008922459674067795\n",
      "Batch 69, Total Loss: 0.002147760009393096\n",
      "Batch 70, Total Loss: 0.0008729433757252991\n",
      "Batch 71, Total Loss: 0.0002829492441378534\n",
      "Batch 72, Total Loss: 0.0007069715647958219\n",
      "Batch 73, Total Loss: 0.0005209479131735861\n",
      "Batch 74, Total Loss: 0.0034888575319200754\n",
      "Batch 75, Total Loss: 0.005028202664107084\n",
      "Batch 76, Total Loss: 0.0018557377625256777\n",
      "Batch 77, Total Loss: 0.0004408142121974379\n",
      "Batch 78, Total Loss: 0.01137002557516098\n",
      "Batch 79, Total Loss: 0.0013768814969807863\n",
      "Batch 80, Total Loss: 0.0012672500452026725\n",
      "Batch 81, Total Loss: 0.005792857147753239\n",
      "Batch 82, Total Loss: 0.0014889470767229795\n",
      "Batch 83, Total Loss: 0.0020554184447973967\n",
      "Batch 84, Total Loss: 0.0008074230863712728\n",
      "Batch 85, Total Loss: 0.0001550777960801497\n",
      "Batch 86, Total Loss: 0.0010719787096604705\n",
      "Batch 87, Total Loss: 4.002631612820551e-05\n",
      "Batch 88, Total Loss: 0.006959754042327404\n",
      "Batch 89, Total Loss: 0.0006474044057540596\n",
      "Batch 90, Total Loss: 0.0009951289976015687\n",
      "Batch 91, Total Loss: 0.002298111794516444\n",
      "Batch 92, Total Loss: 0.000153091037645936\n",
      "Batch 93, Total Loss: 0.0008168424246832728\n",
      "Batch 94, Total Loss: 7.983764226082712e-05\n",
      "Batch 95, Total Loss: 0.004342437256127596\n",
      "Batch 96, Total Loss: 6.910812953719869e-05\n",
      "Batch 97, Total Loss: 0.000730927218683064\n",
      "Batch 98, Total Loss: 0.0009041524608619511\n",
      "Batch 99, Total Loss: 0.002106908243149519\n",
      "Batch 0, Total Loss: 0.0018746898276731372\n",
      "Batch 1, Total Loss: 0.0018226944375783205\n",
      "Batch 2, Total Loss: 0.00938216783106327\n",
      "Batch 3, Total Loss: 0.0007100992952473462\n",
      "Batch 4, Total Loss: 0.012503325939178467\n",
      "Batch 5, Total Loss: 0.007443244569003582\n",
      "Batch 6, Total Loss: 0.0005883712437935174\n",
      "Batch 7, Total Loss: 0.0009720426169224083\n",
      "Batch 8, Total Loss: 0.0007620443939231336\n",
      "Batch 9, Total Loss: 0.002693325746804476\n",
      "Batch 10, Total Loss: 0.01778659224510193\n",
      "Batch 11, Total Loss: 0.001987823285162449\n",
      "Batch 12, Total Loss: 0.012715219520032406\n",
      "Batch 13, Total Loss: 0.027918698266148567\n",
      "Batch 14, Total Loss: 0.0836118757724762\n",
      "Batch 15, Total Loss: 0.02451658621430397\n",
      "Batch 16, Total Loss: 0.014908969402313232\n",
      "Batch 17, Total Loss: 0.006106731481850147\n",
      "Batch 18, Total Loss: 0.006760583259165287\n",
      "Batch 19, Total Loss: 0.006807046476751566\n",
      "Batch 20, Total Loss: 0.003881781594827771\n",
      "Batch 21, Total Loss: 0.010097065009176731\n",
      "Batch 22, Total Loss: 0.005559501703828573\n",
      "Batch 23, Total Loss: 0.0018601107876747847\n",
      "Batch 24, Total Loss: 0.002616525162011385\n",
      "Batch 25, Total Loss: 0.006735608913004398\n",
      "Batch 26, Total Loss: 0.008193210698664188\n",
      "Batch 27, Total Loss: 0.01521043200045824\n",
      "Batch 28, Total Loss: 0.024020707234740257\n",
      "Batch 29, Total Loss: 0.0031534917652606964\n",
      "Batch 30, Total Loss: 0.0061133848503232\n",
      "Batch 31, Total Loss: 0.020237701013684273\n",
      "Batch 32, Total Loss: 0.004101899452507496\n",
      "Batch 33, Total Loss: 0.0023880808148533106\n",
      "Batch 34, Total Loss: 0.011398417875170708\n",
      "Batch 35, Total Loss: 0.0021151243709027767\n",
      "Batch 36, Total Loss: 0.009223787114024162\n",
      "Batch 37, Total Loss: 0.0036047096364200115\n",
      "Batch 38, Total Loss: 0.0020689875818789005\n",
      "Batch 39, Total Loss: 0.006164303049445152\n",
      "Batch 40, Total Loss: 0.011608939617872238\n",
      "Batch 41, Total Loss: 0.0003806791501119733\n",
      "Batch 42, Total Loss: 0.006093874573707581\n",
      "Batch 43, Total Loss: 0.007251352537423372\n",
      "Batch 44, Total Loss: 0.006860839668661356\n",
      "Batch 45, Total Loss: 0.001218997873365879\n",
      "Batch 46, Total Loss: 0.010210377164185047\n",
      "Batch 47, Total Loss: 0.008393803611397743\n",
      "Batch 48, Total Loss: 0.0003479808510746807\n",
      "Batch 49, Total Loss: 0.010681192390620708\n",
      "Batch 50, Total Loss: 0.00029118102975189686\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    num_batches = 100\n",
    "    boundary_loss_weight = 0  # Adjust as necessary\n",
    "\n",
    "    for batch_idx, batch in enumerate(dg.duffing_generator_batch(num_batches)):\n",
    "        x_combined, y_batch = batch\n",
    "        x_combined, y_batch = x_combined.to(device), y_batch.to(device)\n",
    "    \n",
    "        x_boundary = torch.tensor([[X_BOUNDARY]], dtype=torch.float32).to(device)\n",
    "        target_value = torch.tensor([[F_BOUNDARY]], dtype=torch.float32).to(device)\n",
    "    \n",
    "        optimizer.zero_grad()\n",
    "    \n",
    "        # Slice out a small number of points and corresponding parameters\n",
    "        indices = torch.arange(0, 10, 1000).to(device)  # Adjust the slicing as needed\n",
    "        x_data = x_combined[indices, 0:1]\n",
    "        params_data = x_combined[indices, 1:]\n",
    "        y_data = y_batch[indices]\n",
    "        # Compute physics loss\n",
    "        x_time = x_combined[:, 0:1]\n",
    "        x_params = x_combined[:, 1:]\n",
    "    \n",
    "        # Model prediction for data loss\n",
    "        yh = model(x_data, params_data)\n",
    "        mse_loss = torch.mean((yh - y_data) ** 2)\n",
    "        def boundary_condition_loss(model, x_boundary, target_value):\n",
    "            # Create a tensor of zeros with the correct shape for the parameters\n",
    "            zero_params = torch.zeros_like(x_boundary).expand(-1, 5)  # Adjust '5' if the number of parameters is different\n",
    "        \n",
    "            model_output = model(x_boundary, x_params)\n",
    "            return torch.mean((model_output - target_value) ** 2)\n",
    "     \n",
    "        # Compute boundary loss\n",
    "        boundary_loss = boundary_condition_loss(model, x_boundary, target_value)\n",
    "  \n",
    "        loss_physics = loss_instance.physics_loss(model, x_time, x_params, device)\n",
    "    \n",
    "        # Combine losses\n",
    "        total_loss = mse_loss + boundary_loss_weight * boundary_loss + loss_physics\n",
    "    \n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "        print(f\"Batch {batch_idx}, Total Loss: {total_loss.item()}\")\n",
    "    # Visualization every 10 epochs\n",
    "    if (epoch+ 1) % 10 == 0:\n",
    "        model.eval()  # Set the model to evaluation mode\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            yh = model(x_time, x_params).detach().cpu().numpy()\n",
    "            time_points = x_combined[:, 0].cpu().numpy()  # Extract time points from combined input\n",
    "            plt.figure(figsize=(10, 5))\n",
    "            plt.plot(time_points, y_batch.cpu().numpy(), label='Ground Truth')\n",
    "            plt.plot(time_points, yh, label='Neural Network Output')\n",
    "            plt.scatter(time_points, y_batch.cpu().numpy(), color='red', label='Training Points')\n",
    "            plt.xlabel(\"Time\")\n",
    "            plt.ylabel(\"Value\")\n",
    "            plt.legend()\n",
    "            plt.title(f\"Model Output vs Ground Truth at Epoch {epoch+1}\")\n",
    "            plt.show()\n",
    "\n",
    "    # Optional: Add validation logic here\n",
    "\n",
    "# Optional: Save the model after training\n",
    "# torch.save(model.state_dict(), 'model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f62bc8b9-2944-4f9d-bf9f-edff45b5c985",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40799d56-f7cf-4dc0-b80f-ce38f287d4e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the dimensions or use actual test data for test_input\n",
    "test_input_dimensions = (100, 6)  # Example dimensions, modify as necessary\n",
    "test_input = torch.randn(test_input_dimensions, dtype=torch.double, requires_grad=True)  # Create a random tensor with requires_grad set to True\n",
    "\n",
    "# Convert test input to Float\n",
    "test_input = test_input.float()\n",
    "\n",
    "# Move model and test_input to the same device\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "test_input = test_input.to(device)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Test function for gradcheck\n",
    "def test_func(input):\n",
    "    return model(input)\n",
    "# Perform the gradcheck with the modified test input\n",
    "gradcheck_passed = gradcheck(test_func, test_input, eps=1e-5, atol=1e-4)\n",
    "print(\"Gradcheck passed:\", gradcheck_passed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25c8aa14-0cb3-46b5-abdd-13f7952877ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create a sample input tensor with batch size greater than 1\n",
    "sample_input = torch.randn(2, N_INPUT, requires_grad=True)\n",
    "\n",
    "# Move model and sample_input to the same device\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "sample_input = sample_input.to(device)\n",
    "\n",
    "# Forward pass\n",
    "output = model(sample_input)\n",
    "\n",
    "# Sample loss computation (replace with a loss appropriate for your model)\n",
    "sample_loss = output.sum()\n",
    "\n",
    "# Backward pass\n",
    "sample_loss.backward()\n",
    "\n",
    "# Check if gradients are computed for the input\n",
    "print(\"Gradients for the input tensor:\", sample_input.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05a5951d-d23e-4e2a-a5e1-78b2205df0eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc93f942-8bd1-4df8-8473-a683ad77e25c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
