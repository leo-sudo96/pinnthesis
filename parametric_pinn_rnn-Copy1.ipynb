{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b4591d13-11ac-47db-a0f2-28be2230b035",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from scipy.integrate import odeint\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.integrate import odeint\n",
    "from torch import nn, autograd\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9180a41e-fd6d-4f1d-9356-04b67a75d37b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdaptedLSTM(nn.Module):\n",
    "    def __init__(self, n_time_features, n_param_features, n_hidden, n_layers):\n",
    "        super(AdaptedLSTM, self).__init__()\n",
    "        \n",
    "        # Number of input features for the LSTM is the sum of time features and parameter features\n",
    "        self.lstm_input_features = n_time_features + n_param_features\n",
    "        \n",
    "        # LSTM Layer\n",
    "        self.lstm = nn.LSTM(input_size=self.lstm_input_features, hidden_size=n_hidden, num_layers=n_layers, batch_first=True)\n",
    "        \n",
    "        # Output layer\n",
    "        self.output_layer = nn.Linear(n_hidden, 1)\n",
    "        \n",
    "        # Activation function\n",
    "        self.activation = nn.Tanh()\n",
    "\n",
    "    def forward(self, x, params):\n",
    "        # Assuming x is of shape [batch_size, seq_len, n_time_features]\n",
    "        # And params is of shape [batch_size, n_param_features]\n",
    "        x = x.unsqueeze(1)  # Adds seq_len dimension\n",
    "        \n",
    "        # Since params is [500, 5], we also need to adjust it for concatenation\n",
    "        # We want params to be [500, 1, 5] to match x's new shape for concatenation\n",
    "        params = params.unsqueeze(1)  # Adds seq_len dimension\n",
    "        \n",
    "        # Now x_combined shape will be [500, 1, 6] after concatenation\n",
    "        x_combined = torch.cat((x, params), dim=2)\n",
    "        \n",
    "        # LSTM layer\n",
    "        lstm_out, (hn, cn) = self.lstm(x_combined)\n",
    "        \n",
    "        # Apply activation function to the output of the last timestep\n",
    "        final_output = self.activation(lstm_out[:, -1, :])\n",
    "        \n",
    "        # Output layer without activation (assuming a regression task)\n",
    "        final_output = self.output_layer(final_output)\n",
    "        return final_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "234a6824-18a4-48fa-9573-4860a0c73531",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class AdaptedGRU(nn.Module):\n",
    "    def __init__(self, n_time_features, n_param_features, n_hidden, n_layers):\n",
    "        super(AdaptedGRU, self).__init__()\n",
    "        \n",
    "        # Number of input features for the GRU is the sum of time features and parameter features\n",
    "        self.gru_input_features = n_time_features + n_param_features\n",
    "        \n",
    "        # GRU Layer\n",
    "        self.gru = nn.GRU(input_size=self.gru_input_features, hidden_size=n_hidden, num_layers=n_layers, batch_first=True)\n",
    "        \n",
    "        # Output layer\n",
    "        self.output_layer = nn.Linear(n_hidden, 1)\n",
    "        \n",
    "        # Activation function\n",
    "        self.activation = nn.Tanh()\n",
    "\n",
    "    def forward(self, x, params):\n",
    " \n",
    "        x = x.unsqueeze(1)  # Adds seq_len dimension\n",
    "   \n",
    "        params = params.unsqueeze(1)  # Adds seq_len dimension\n",
    "        \n",
    "    \n",
    "        # Concatenate time and parameter features for each timestep\n",
    "        x_combined = torch.cat((x, params), dim=2)\n",
    "      \n",
    "\n",
    "        # GRU layer\n",
    "        gru_out, hn = self.gru(x_combined)\n",
    "        \n",
    "        # Apply activation function to the output of the last timestep\n",
    "        final_output = self.activation(gru_out[:, -1, :])\n",
    "        \n",
    "        # Output layer without activation (assuming a regression task)\n",
    "        final_output = self.output_layer(final_output)\n",
    "        return final_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3b4fd91-5611-4708-ab6a-1fca5f87fec6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7d694e84-e127-405a-96da-eb47a2aaf3a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdaptedRNN(nn.Module):\n",
    "    def __init__(self, n_time_features, n_param_features, n_hidden, n_layers):\n",
    "        super(AdaptedRNN, self).__init__()\n",
    "        \n",
    "        # Number of input features for the RNN is the sum of time features and parameter features\n",
    "        self.rnn_input_features = n_time_features + n_param_features\n",
    "        \n",
    "        # RNN Layer\n",
    "        self.rnn = nn.RNN(input_size=self.rnn_input_features, hidden_size=n_hidden, num_layers=n_layers, batch_first=True)\n",
    "        \n",
    "        # Output layer\n",
    "        self.output_layer = nn.Linear(n_hidden, 1)\n",
    "        \n",
    "        # Activation function\n",
    "        self.activation = nn.Tanh()\n",
    "\n",
    "    def forward(self, x, params):\n",
    "        # Reshape x to add a sequence length dimension of 1\n",
    "        # If x is [500, 1], we reshape it to [500, 1, 1] to match [batch_size, seq_len, n_time_features]\n",
    "        x = x.unsqueeze(1)  # Adds seq_len dimension\n",
    "        \n",
    "        # Since params is [500, 5], we also need to adjust it for concatenation\n",
    "        # We want params to be [500, 1, 5] to match x's new shape for concatenation\n",
    "        params = params.unsqueeze(1)  # Adds seq_len dimension\n",
    "        \n",
    "        # Now x_combined shape will be [500, 1, 6] after concatenation\n",
    "        x_combined = torch.cat((x, params), dim=2)\n",
    "        \n",
    "        # Process with RNN as before\n",
    "        rnn_out, _ = self.rnn(x_combined)\n",
    "        final_output = rnn_out[:, -1, :]  # Taking the last timestep output\n",
    "        final_output = self.activation(final_output)\n",
    "        final_output = self.output_layer(final_output)\n",
    "        return final_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "939add40-9e1c-40a1-b7d3-afac65c2d809",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdaptedFCN(nn.Module):\n",
    "    def __init__(self, n_time_features, n_param_features, n_hidden, n_layers):\n",
    "        super(AdaptedFCN, self).__init__()\n",
    "        \n",
    "        # Number of input features is the sum of time features and parameter features\n",
    "        self.input_features = n_time_features + n_param_features\n",
    "        \n",
    "        # Input layer takes combined time and parameters\n",
    "        self.input_layer = nn.Linear(self.input_features, n_hidden)\n",
    "        \n",
    "        # Hidden layers\n",
    "        self.hidden_layers = nn.ModuleList()\n",
    "        for _ in range(n_layers - 1):\n",
    "            self.hidden_layers.append(nn.Linear(n_hidden, n_hidden))\n",
    "        \n",
    "        # Output layer\n",
    "        self.output_layer = nn.Linear(n_hidden, 1)\n",
    "        \n",
    "        # Activation function\n",
    "        self.activation = nn.Tanh()  # Using Tanh as an example; adjust as needed\n",
    "\n",
    "    def forward(self, x, params):\n",
    "        # Concatenate time and parameter features\n",
    "        x_combined = torch.cat((x, params), dim=1)\n",
    "        \n",
    "        # Input layer with activation\n",
    "        x = self.activation(self.input_layer(x_combined))\n",
    "        \n",
    "        # Hidden layers with activation\n",
    "        for layer in self.hidden_layers:\n",
    "            x = self.activation(layer(x))\n",
    "        \n",
    "        # Output layer without activation (assuming a regression task)\n",
    "        x = self.output_layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8efc8910-1ce7-446e-90ee-e7152961524a",
   "metadata": {},
   "outputs": [],
   "source": [
    "    def duffing_generator_batch( num_batches, x):\n",
    "        params_list = []  # To store parameters tensors for each batch\n",
    "        y_physics_list = []  # To store the y_physics tensors for each batch\n",
    "\n",
    "        for _ in range(num_batches):\n",
    "            # Randomly generate parameters\n",
    "            a = random.uniform(-2, 2)\n",
    "            b = random.uniform(0, 3)\n",
    "            d = random.uniform(0, 0.5)\n",
    "            gamma = random.uniform(0, 3)\n",
    "            w = random.uniform(0, 5)\n",
    "            y0 = [0, random.uniform(-1, 1)]\n",
    "            # Duffing differential equation solver setup\n",
    "            def duffing(y, t):\n",
    "                y0, y1 = y\n",
    "                dydt = [y1, -d * y1 - a * y0 - b * y0**3 + gamma * np.cos(w * t)]\n",
    "                return dydt\n",
    "\n",
    "            # Initial conditions and solving the ODE\n",
    "            y0 = [0, 0]\n",
    "            sol = odeint(duffing, y0, x.cpu().squeeze().numpy())  # Ensure x is compatible with odeint\n",
    "            y = torch.tensor(sol[:, 0], dtype=torch.float32).view(-1, 1)  # y_physics for one batch\n",
    "\n",
    "            y_physics_list.append(y)\n",
    "\n",
    "            # Handling parameters similarly if needed\n",
    "            params = torch.tensor([d, a, b, gamma, w], dtype=torch.float32).view(1, -1).repeat(x.size(0), 1)\n",
    "            # Creating y0_tensor for the second entry of y0 and concatenating it to the params tensor\n",
    "            y0_tensor = torch.tensor([y0[1]], dtype=torch.float32).view(1, -1).repeat(x.size(0), 1)  # Only interested in the second entry of y0\n",
    "            combined_params = torch.cat((params, y0_tensor), dim=1)\n",
    "            params_list.append(combined_params)\n",
    "       \n",
    "\n",
    "            # Stack the list of tensors to create a batch dimension\n",
    "            params_tensor = torch.stack(params_list, dim=0)\n",
    "    \n",
    "  \n",
    "  \n",
    "            y_physics_tensor = torch.stack(y_physics_list, dim=0)  # Shape: [num_batches, x.size(0), 1]\n",
    "\n",
    "    \n",
    "            return params_tensor, y_physics_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fd1872df-5e7c-4cb9-8838-db76c39dbd37",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "expand(torch.FloatTensor{[1, 1, 1, 5000, 6]}, size=[-1, 1, -1]): the number of sizes provided (3) must be greater or equal to the number of dimensions in the tensor (5)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 47\u001b[0m\n\u001b[1;32m     44\u001b[0m selected_params \u001b[38;5;241m=\u001b[39m params_tensor\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# Add the seq_len dimension, [batch_size, 1, n_param_features]\u001b[39;00m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;66;03m# Model prediction\u001b[39;00m\n\u001b[0;32m---> 47\u001b[0m yh \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43mselected_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;66;03m# Compute the data loss by comparing the model output with the training data\u001b[39;00m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;66;03m# Assuming your linear layer is named `layer`\u001b[39;00m\n\u001b[1;32m     51\u001b[0m loss1 \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmean((yh \u001b[38;5;241m-\u001b[39m y_data)\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/thesis/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/thesis/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[10], line 20\u001b[0m, in \u001b[0;36mAdaptedGRU.forward\u001b[0;34m(self, x, params)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, params):\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;66;03m# Assuming params needs to be expanded to match x's seq_len for concatenation\u001b[39;00m\n\u001b[1;32m     19\u001b[0m     seq_len \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 20\u001b[0m     params_expanded \u001b[38;5;241m=\u001b[39m \u001b[43mparams\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexpand\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseq_len\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Now [batch_size, seq_len, n_param_features]\u001b[39;00m\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;66;03m# Concatenate time and parameter features for each timestep\u001b[39;00m\n\u001b[1;32m     23\u001b[0m     x_combined \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat((x, params_expanded), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)  \u001b[38;5;66;03m# Now [batch_size, seq_len, n_time_features + n_param_features]\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: expand(torch.FloatTensor{[1, 1, 1, 5000, 6]}, size=[-1, 1, -1]): the number of sizes provided (3) must be greater or equal to the number of dimensions in the tensor (5)"
     ]
    }
   ],
   "source": [
    "# Your setup code, including model and optimizer initialization\n",
    "x = torch.linspace(0, 10, 5000).view(-1,1)\n",
    "x_physics = torch.linspace(0, 10, 3000).view(-1, 1).requires_grad_(True)\n",
    "\n",
    "torch.manual_seed(123)\n",
    "#model = AdaptedFCN(1, 5, 32, 8)  \n",
    "#model = AdaptedRNN(1, 5, 32, 8)  \n",
    "#model = AdaptedLSTM(1,6,32,8)\n",
    "model = AdaptedGRU(1,6,32,8)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=20048, gamma=0.5)\n",
    "\n",
    "\n",
    "data_processed = 0  # Initialize a counter\n",
    "variance_threshold = 0.000001  # Acceptable variance threshold\n",
    "loss_change_threshold = 1e-8  # Acceptable change in loss\n",
    "check_interval = 100  # How often to check the criteria (in epochs)\n",
    "num_batches = 128\n",
    "loss_history = []  # To store loss values for change calculation\n",
    "variances = []  # To track variance at each checkpoint\n",
    "epochs_with_checks = []  # To track epochs when checks are performed\n",
    "\n",
    "for i in range(18000):\n",
    "    epoch_losses = []  # To store losses for this epoch\n",
    "    epoch_differences = []  # To store prediction differences for variance calculation\n",
    "    \n",
    "    for batch in range(num_batches):\n",
    "        optimizer.zero_grad()\n",
    "        params_tensor,y_tensor= duffing_generator_batch( num_batches, x)\n",
    "        selected_params = params_tensor[0, :1, :].repeat(x.size(0), 1)  # Adjusted to have shape [500, 5]\n",
    "        y_data = y_tensor[:, 0:2000:5] \n",
    "        x_data = x[0:2000:5] \n",
    "        data_processed += x_data.shape[0]\n",
    "        d, a, b, gamma, w= params_tensor[:, 0], params_tensor[:, 1], params_tensor[:, 2], params_tensor[:, 3], params_tensor[:, 4]\n",
    "        selected_params = params_tensor.repeat(x_data.size(0), 1, 1)  # This should now correctly shape selected_params as [500, 6] for x_data\n",
    "        \n",
    "        yh = model(x_data.view(-1, 1, 1),selected_params)\n",
    "        \n",
    "        # Compute the data loss by comparing the model output with the training data\n",
    "        # Assuming your linear layer is named `layer`\n",
    "        loss1 = torch.mean((yh - y_data)**2)\n",
    "        physics_params = params_tensor[0, :1, :].repeat(x_physics.size(0), 1) \n",
    "        # Compute the physics loss by enforcing the differential equation\n",
    "        yhp = model(x_physics,physics_params)\n",
    "        dy_pred = autograd.grad(yhp, x_physics, torch.ones_like(yhp), create_graph=True)[0]\n",
    "        d2y_pred = autograd.grad(dy_pred, x_physics, torch.ones_like(dy_pred), create_graph=True)[0]\n",
    "            \n",
    "        physics = d2y_pred + d * dy_pred + a * yhp + b * torch.pow(yhp, 3) - gamma * torch.cos(w * x_physics)\n",
    "        loss_physics = (1e-4) * torch.mean(physics**2)\n",
    "  \n",
    "        # Compute the total loss as the sum of the data loss, the physics loss, and the boundary loss\n",
    "        total_loss = loss1 + loss_physics \n",
    "            \n",
    "        # Update the model parameters using backpropagation and gradient descent\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "        y_tensor_squeezed = y_tensor.squeeze()\n",
    "            # Adjust learning rate based on scheduler\n",
    "        scheduler.step()\n",
    "        # Collect differences for variance calculation\n",
    "        differences = (yh - y_data).detach().cpu().numpy()  # Assuming y_data and yh are tensors\n",
    "        epoch_differences.extend(differences.flatten())\n",
    "        epoch_losses.append(total_loss.item())  # Store loss for later analysis\n",
    "\n",
    "\n",
    "        # Calculate the mean loss for the epoch\n",
    "    mean_loss = np.mean(epoch_losses)\n",
    "    loss_history.append(mean_loss)\n",
    "    \n",
    "    # Check variance and loss change at specified intervals\n",
    "    if (i + 1) % check_interval == 0:\n",
    "        epoch_variance = np.var(epoch_differences)\n",
    "        variances.append(epoch_variance)\n",
    "        epochs_with_checks.append(i + 1)\n",
    "\n",
    "        # Calculate the absolute change in loss compared to the last checkpoint\n",
    "        if len(loss_history) > 1:\n",
    "            loss_change = abs(loss_history[-1] - loss_history[-2])\n",
    "        else:\n",
    "            loss_change = float('inf')  # Initial loss change is set to inf to ensure it doesn't trigger stopping\n",
    "\n",
    "        print(f\"Epoch {i+1}, Variance: {epoch_variance}, Loss Change: {loss_change}\")\n",
    "        \n",
    "        # Check if both the variance and loss change criteria are met\n",
    "        if epoch_variance < variance_threshold and loss_change < loss_change_threshold:\n",
    "            print(\"Both variance and loss change criteria met, consider adjusting training strategy.\")\n",
    "            # Save the model\n",
    "            model_save_path = \"/devel/thesis/rnn_pinnthesis/pinn_model.pth\"\n",
    "            torch.save(model.state_dict(), model_save_path)\n",
    "            print(f\"Model saved to {model_save_path}\")\n",
    "            break  # Exit training loop\n",
    "\n",
    "    # Your original plotting code here for every few epochs\n",
    "    # Plot the results every 150 steps\n",
    "    if (i+1) % 10 == 0:   \n",
    "        # Print the loss value after each step\n",
    "        print(\"Loss at Step\", i+1, \":\", total_loss.item())\n",
    "        print(f\"Epoch {i+1}, Total Data Processed: {data_processed}\")\n",
    "        yh = model(x,selected_params).detach().numpy()\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.plot(x.numpy(), y_tensor_squeezed.numpy(), label='Ground Truth')\n",
    "        plt.plot(x.numpy(), yh, label='Neural Network Output')\n",
    "        plt.scatter(x_data.numpy(), y_data.numpy(), color='red', label='Training points')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "# After training, plot the variance threshold chart\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(epochs_with_checks, variances, label='Prediction Variance')\n",
    "plt.axhline(y=variance_threshold, color='r', linestyle='-', label='Variance Threshold')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Variance')\n",
    "plt.title('Model Prediction Variance Over Time')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "146aecd7-9fb5-4b6e-ac38-6d2432593212",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "'(' was never closed (698879034.py, line 84)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[22], line 84\u001b[0;36m\u001b[0m\n\u001b[0;31m    rint(f\"Epoch {i+1}: Cumulative Data Processed = {data_processed}, Data Processed This Epoch = {epoch_data_processed}\"\u001b[0m\n\u001b[0m        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m '(' was never closed\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "x = torch.linspace(0, 1, 500).view(-1,1)\n",
    "x_physics = torch.linspace(0, 1, 30).view(-1, 1).requires_grad_(True).to(device)\n",
    "num_batches = 128\n",
    "torch.manual_seed(123)\n",
    "model = AdaptedFCN(1, 5, 256, 32)  # Assuming AdaptedFCN is defined elsewhere\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.5)\n",
    "model = model.to(device)\n",
    "# Constants\n",
    "X_BOUNDARY = torch.tensor([0.0], device=device).view(-1, 1)\n",
    "F_BOUNDARY = torch.tensor([0.0], device=device).view(-1, 1)\n",
    "\n",
    "# Training Variables\n",
    "data_processed = 0\n",
    "variance_threshold = 0.000001\n",
    "loss_change_threshold = 1e-8\n",
    "check_interval = 100\n",
    "num_batches = 128\n",
    "data_processed_per_epoch = []  # This will track data processed per epoch\n",
    "loss_history = []\n",
    "variances = []\n",
    "epochs_with_checks = []\n",
    "\n",
    "# Training Loop\n",
    "for i in range(6000):\n",
    "    epoch_losses = []\n",
    "    epoch_differences = []\n",
    "    \n",
    "    for batch in range(num_batches):\n",
    "        optimizer.zero_grad()\n",
    "        params_tensor, y_tensor = duffing_generator_batch(num_batches, x)\n",
    "        params_tensor = params_tensor.to(device)\n",
    "        y_tensor = y_tensor.to(device)\n",
    "        \n",
    "        selected_params = params_tensor[0, :1, :].repeat(x.size(0), 1).to(device)  # Ensure selected_params is on the correct device\n",
    "        y_data = y_tensor[:, 0:200:20]\n",
    "        x_data = x[0:200:20].to(device)  # Ensure x_data is also on the correct device before passing to model\n",
    "        \n",
    "        yh = model(x_data, selected_params[0:200:20])\n",
    "        # Compute the data loss\n",
    "        loss1 = torch.mean((yh - y_data)**2)\n",
    "        \n",
    "        # Physics parameters, ensuring they're on the correct device\n",
    "        physics_params = params_tensor[0, :1, :].repeat(x_physics.size(0), 1)\n",
    "        physics_params = physics_params.to(device)  # Explicitly ensuring on correct device\n",
    "        yhp = model(x_physics, physics_params)\n",
    "        \n",
    "        # Extracting individual parameters for the physics calculation\n",
    "        d, a, b, gamma, w = params_tensor[:, 0], params_tensor[:, 1], params_tensor[:, 2], params_tensor[:, 3], params_tensor[:, 4]\n",
    "        \n",
    "        dy_pred = autograd.grad(yhp, x_physics, torch.ones_like(yhp), create_graph=True)[0]\n",
    "        d2y_pred = autograd.grad(dy_pred, x_physics, torch.ones_like(dy_pred), create_graph=True)[0]\n",
    "        \n",
    "        physics = d2y_pred + d * dy_pred + a * yhp + b * torch.pow(yhp, 3) - gamma * torch.cos(w * x_physics)\n",
    "        loss_physics = (1e-4) * torch.mean(physics**2)\n",
    "        \n",
    "        boundary_params = params_tensor[0, :1, :].repeat(1, 1)\n",
    "        yh_boundary = model(X_BOUNDARY, boundary_params)\n",
    "        loss_boundary = torch.mean((yh_boundary - F_BOUNDARY)**2)\n",
    "        \n",
    "        total_loss = loss1 + loss_physics + loss_boundary\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        batch_size = x_data.shape[0]  # Assuming x_data is your input batch tensor\n",
    "        epoch_data_processed += batch_size\n",
    "        data_processed += batch_size\n",
    "        differences = (yh - y_data).detach().cpu().numpy()\n",
    "        epoch_differences.extend(differences.flatten())\n",
    "        epoch_losses.append(total_loss.item())\n",
    "\n",
    "    mean_loss = np.mean(epoch_losses)\n",
    "    loss_history.append(mean_loss)\n",
    "    \n",
    "    if (i + 1) % check_interval == 0:\n",
    "        epoch_variance = np.var(epoch_differences)\n",
    "        variances.append(epoch_variance)\n",
    "        epochs_with_checks.append(i + 1)\n",
    "        loss_change = abs(loss_history[-1] - loss_history[-2]) if len(loss_history) > 1 else float('inf')\n",
    "        print(f\"Epoch {i+1}, Variance: {epoch_variance}, Loss Change: {loss_change}\")\n",
    "        rint(f\"Epoch {i+1}: Cumulative Data Processed = {data_processed}, Data Processed This Epoch = {epoch_data_processed}\"\n",
    "        if epoch_variance < variance_threshold and loss_change < loss_change_threshold:\n",
    "            print(\"Criteria met, stopping training.\")\n",
    "            model_save_path = \"/devel/thesis/rnn_pinnthesismodel.pth\"\n",
    "            torch.save(model.state_dict(), model_save_path)\n",
    "            break\n",
    "\n",
    "    if (i+1) % 10 == 0:\n",
    "        print(\"Loss at Step\", i+1, \":\", total_loss.item())\n",
    "        print(f\"Epoch {i+1}, Total Data Processed: {data_processed}\")\n",
    "    \n",
    "        # Convert tensors to CPU for plotting if they are on a GPU\n",
    "        x_data_cpu = x_data.detach().cpu().numpy().squeeze()  # Squeeze to ensure correct dimensions\n",
    "        y_data_cpu = y_data.detach().cpu().numpy().squeeze()  # Squeeze to ensure correct dimensions\n",
    "        yh_cpu = yh.detach().cpu().numpy().squeeze()  # Also squeeze yh for consistency\n",
    "    \n",
    "        plt.figure(figsize=(10, 5))\n",
    "        # Ensure dimension compatibility in the plot by using squeezed arrays\n",
    "        plt.scatter(x_data_cpu, y_data_cpu,  color='red', label='Training points')\n",
    "        plt.plot(x.numpy(), y_tensor_squeezed.detach().cpu().numpy(), label='Ground Truth')\n",
    "        plt.plot(x_data_cpu, yh_cpu, label='Model Output')\n",
    "        plt.title(\"Model Output vs. Training Data\")\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "# After the training loop\n",
    "# Plot the variance threshold chart\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(epochs_with_checks, variances, 'o-', label='Prediction Variance')\n",
    "plt.axhline(y=variance_threshold, color='r', linestyle='--', label='Variance Threshold')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Variance')\n",
    "plt.title('Model Prediction Variance Over Time')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35a72577-02e6-4399-8e43-449363040e0a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
